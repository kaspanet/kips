```
  KIP: 13
  Layer: Consensus (hard fork), Block Size
  Title: Transient Storage Handling
  Author: Michael Sutton <msutton@cs.huji.ac.il>
          coderofstuff
  Status: draft
```

We propose tracking the transient storage consumption of transactions as a `mass` and bounding such consumption to a reasonable size for node operators.

# Motivation

Payloads have been enabled in Testnet 11 which is running at 10 blocks per second (BPS). Payload bytes are charged at a gram per byte. At 10 BPS and with the current block mass limit of `500,000` grams, through the use of massive payloads, it's possible that storage within the pruning period can consume close to 1TB of data. To demonstrate this, we use the following calculation:

```
// Given:
block_mass_limit = 500,000
testnet_11_pruning_depth = 1,119,290
testnet_11_finality_depth = 432,000
bytes_per_gb = 1,000,000,000
```

```
// Assuming every block mass is exactly 1 byte:
worst_case_usage
= ((testnet_11_pruning_depth + testnet_11_finality_depth) * block_mass_limit) / bytes_per_gb;
= ((1,119,290 + 432,000) * 500,000) / 1,000,000,000
= 775 GB
```

Requiring node operators and common users who run nodes to have close to 1TB free to run a node is a tall ask. We must consider reducing this worst-case usage.

## Typical Transactions

These are the most common kinds of tranactions that network users make.

A typical 1:2 transaction (1 input, 2 outputs)'s consumption looks like this

|Field|Data|
| --- | --- |
| Size | 316 bytes |
| Actual Mass | 2036 grams |
| TPB* | 245 |
| Total Block Size** | 77,420 bytes |

*The transactions per block if it were completely filled with transactions like this only

**Total Block Size consumed if the block was filled with transactions like this only

Similarly 2:2 transactions (2 inputs, 2 outputs)'s consumption looks like this

|Field|Data|
| --- | --- |
| Size | 434 bytes |
| Actual Mass | 3154 grams |
| TPB* | 158 |
| Total Block Size** | 68,572 bytes |

Finally a typical KRC20 transaction is usually either a 1:1 or 1:2 transaction. It's signature length is roughly 261 bytes.

|Field|Data|
| --- | --- |
| Size | 429 bytes |
| Actual Mass | 1819 grams |
| TPB* | 274 |
| Total Block Size** | 117,546 bytes |

Plugging these sizes with the exact max pruning length (pruning + finality depth) of 10BPS, we get worst case disk sizes of 120GB, 106GB and 182GB respectively. 

If we round it up to 125kb per block you get worst case usage of 193GB, which is much more reasonable and accessible than the initially calculated 775GB worst case requirement.

With these figures in mind, we can set the goals to be as follows:
1. Keep the above costs for typical transactions (so we still fit those 150-250 TPB) 
2. Limit block size to 125KB

# Our Proposal

## Consensus Changes

The current `check_block_mass` implementation checks whether the transactions within a block fit within the block mass limit. It combines the current storage mass and compute mass into a single overall representative mass, and uses that max value to determine whether the transactions fit within the limit. We propose the following changes:

### Change 1 - New Mass: Transient Storage mass

Introduce a new mass called `transient_storage_mass`. We will use this to limit the block size to 125KB as follows:
- `transient_storage_mass = transaction_serialized_size(tx) * 4`.
- The `4` above comes directly from trying to reduce the block size to 125KB (500,000 mass / 125,000 bytes = 4 mass / byte)

### Change 2 - Independent Block Mass Tracking
Track the block mass consumption of each of `compute_mass`, `storage_mass` and `transient_storage_mass` independently. This is a relaxation of the current block mass limit check which currently tracks these masses together by getting the `max` of each per transaction. This relaxation allows for better consumption of block space.
- Rationale: with the current scheme, a transaction that is with `(compute mass | storage mass)` of `(490,000 | 1,000)` and another that is `(1,000 | 490,000)` cannot fit together in a single block even though they consume different kinds of resources on the node (one consumes a lot of compute and the other consumes a lot of storage). Relaxing the block mass limit to track each mass independently allows for including both transactons that consume __different__ resources heavily in the same block.

## Mempool Changes

### Change 1 - Fee Rate Keys

The `calculated_transient_storage_mass` will be pre-computed and accessible within the mempool. Fee rate keys will be updated to include `calculated_transient_storage_mass` in it's mass via the `max` operator.

That is, a fee rate `mass = max(compute_mass, storage_mass, transient_storage_mass)`

Optimizing the mempool transaction selection mechanism (tracking each mass independently somehow) is out of scope for this KIP.

### Change 2 - Validations

Anywhere in the mempool where transaction compute mass is validated (such as against standard size), such mass will incorporate `transient_storage_mass` via the `max` operator.

# Backwards compatibility
Breaks consensus rules, requires hardfork
