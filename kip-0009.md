```
  KIP: 9
  Layer: Mempool, P2P, Consensus
  Title: Extended mass formula for dust prevention
  Authors: Michael Sutton <msutton@cs.huji.ac.il>
           Ori Newman <orinewman1@gmail.com>
           Shai Wyborski <shai.wyborski@mail.huji.ac.il>
           Yonatan Sompolinsky
  Status: proposed, implemented by the rust codebase and applied in testnet 11 
```

We propose a mechanism for regulating the growth rate of the UTXO set in both organic and adversarial settings. Our proposal is fundamentally different than existing attempts to mitigate state bloat (via accumulators, demmurage, and so on), as it entails nothing but a simple change to the logic of how the mass of a transaction is computed. In this proposal, we specify the revised formula and its consequences and describe how ecosystem software such as wallets, pools, and exchanges should adapt to this change without affecting the quality of service. We provide an intuitive overview of the remarkable properties of the new mass formula, and defer the formal treatment to a soon-to-be-published preprint. However, we stress that all required properties have been formally verified before writing this proposal.

# Motivation
State bloat is a concern all cryptocurrencies are facing. However, in our case there is an additional, more immediate motivation.

A few months ago, the Kaspa network faced a dust attack that exploited the high throughput to create many minuscule UTXOs, forever increasing the storage costs of Kaspa nodes. The attack was eventually stopped by deploying a mempool patch [2]. While great efforts were made to reduce the consequences of the solution to the network's quality-of-service (QoS), it did impose a limitation on the number of transactions with more outputs than inputs allowed on each block (see [1] for a detailed account of the attack and its resolution). Since many standard transactions are of this form (most typically, a transaction with a single input, a destination output, and a change output), this resulted in a noticeable QoS decrease. While the current effect is quite bearable (increasing the delay of such transactions a dozen of seconds at worst), increased adoption will greatly exacerbate this effect, providing ample motivation to expedite the implementation of a sustainable solution.

# Background

### Transaction mass vs. fees
The capacity of a block is given in a quantity called *mass*. The difference between mass and size is that different types of data are considered "heavier" or "denser", in the sense that they consume more mass per unit. For example, a digital signature of a UTXO used as an input must be *verified*, and since verifying signatures is a computationally costly process, the mass consumed by a digital signature is much larger than the mass of an address specified in an output, despite both having roughly the same size (in bytes). For an exact specification of how mass is calculated, see [here](https://kaspa.aspectron.org/transactions/constraints/mass.html#transaction-mass-limits).

Increasing the mass cost of state-wasting transactions is an effective countermeasure for regulating state growth, as it allows us to control how much each block increases the state (to a limited but sufficient extent, as we will soon see). This approach could be considered an adaptable version of imposing high fees on wasteful transactions, with the following benefits:
1. It does not require an estimate of what constitutes a "high" fee. Instead, the fee is naturally monetized as the value of the blockspace consumed by the transaction.
2. It works in the absence of an active fee market. Even if there are no fees, the mass limits regulate the storage increase.

### Quadratic storage costs via local mass costs
Our overarching goal is to make the cost of wasting some amount of storage *quadratic* in the amount of the storage wasted. That is, taking up ten times more storage takes 100 times more mass, taking up 100 times more storage takes 10000 times more mass, etc.

For reasons we will not get into right now, it follows that the amount of mass required to waste some amount of storage always decreases linearly with the *budget* of the attack. That is, the amount of kas available to the attacker (regardless of how the kas is divided into UTXOs). If we use *growth* to denote the amount of storage wasted by the attack, the best we can hope for is that the cost of the attack is proportional to $\text{growth}^2/\text{budget}$ (the proportionality constant is a global system parameter we will discuss in detail later).

The linear `budget` cofactor is not a bug but an extremely useful feature. It essentially means that the amount of mass the user gets "for free" grows like the square root of their budget. This provides enough leeway to reduce the effect of the mass policy on everyday usage, while still ensuring that the cost of a storage attack (on a fixed budget) increases quadratically.

Designing a mass function providing such quadratic growth is tricky, due to two contrasting desires:
1. Global cost: the cost should be quadratic of the growth *regardless of how the attacker structured his attack*. An attacker can structure his attack in many ways, creating many transactions with intricate dependencies between them, and we must make sure that even the cleverest attacks will ultimately pay a quadratic price.
2. Local computation: the mass of a transaction *must not depend on other transactions*, and must be completely computable just from the data contained in the transaction itself. In particular, our mass formula only depends on the values of the inputs and outputs.

To illustrate why this is tricky, consider attempting the above by setting the mass of a transaction with $I$ inputs and $O$ outputs to be proportional to $(O-I)^2$. While locally it achieves quadratic growth, a clever attacker can increase the size of the UTXO set by $N$ entries by creating $N$ transactions, each with one input and two outputs. The accumulated mass of the attack is linear in $N$ rather than quadratic. In fact, we prove in the research paper that *any* attempt to compute mass based only on the *amount* of inputs and outputs (while ignoring their values) will fail. The reason for this ultimately boils down to the following observation: the mass of a transaction with two outputs of value $V/2$ is the same as that of a transaction (with the same inputs) outputting two inputs of values $\varepsilon V$ and $(1-\varepsilon)V$ for an arbitrarily small $\varepsilon$, and this property can be cleverly abused to cheaply increase the storage.

# Proposal
We propose a new quantity called the `storage_mass`, and refer to the current notion of mass as `compute_mass`. Both notions of mass are crucial -- the former protects the network from storage attacks, while the latter protects it from CPU attacks. Hence, we propose the following total mass formula:
$$\text{mass}(tx) = max\lbrace\text{compute mass}(tx) , \text{storage mass}(tx)\rbrace\text{.}$$
The rest of this section specifies our proposal for defining `storage_mass`.

### A balanced cost function
We model a transaction as two sets of values: the input values $I$ and output values $O$ (strictly speaking $I$ and $O$ are multisets, as the same value can occur several times, but we ignore this technicality here). We use the notation $x^+ = \max\lbrace x,0 \rbrace$. The storage mass is defined as follows:

$$\text{storage mass}(tx) = C\cdot\left(\sum_{o  \in  O} \frac{1}{o} - \frac{|I|^2}{\sum_{v \in I} v}\right)^+\text{.}$$

In this formula, the values of inputs/outputs are measured in dworks (one kas is equal $10^8$ dworks). The parameter $C$ can be roughly thought of as the "conversion rate" between "inverse kaspa" and mass (note that if mass is measured in grams then $C$ has units of grams*dwork, or grams per "inverse dwork". That is, an output of value one dwork should require $C$ grams of mass). Further below, we suggest that $C$ should be set to $10^{12}$ (so an output of a single dwork will require as much mass as two million blocks).

We can rewrite a more "symmetric" version of this formula by introducing the notations $A$ and $H$ for arithmetic and harmonic mean respectively. Namely
$$A\left(I\right)=\frac{1}{\left|I\right|}\sum_{i\in I}i\text{, }\text{ }H\left(O\right)=\left|O\right|/\sum_{o\in O}\frac{1}{o}\text{.}$$

With these notations, the definition of `storage_mass` takes on an arguably more pleasant form:
$$\text{storage mass}(tx) = C\cdot\left(\frac{|O|}{H(O)} - \frac{|I|}{A(I)}\right)^+$$

The idea behind this formula is that a transaction should be *charged* for the storage it requires and *credited* for the storage it frees. However, there is a principal difference between them: the storage charge should reflect how distributed the outputs are, while the credit should only reflect the number of consumed UTXOs and their *total* value. This way, transactions that do not increase the UTXO set much (or even decrease it) but create minuscule inputs are heavily charged, while transactions whose inputs and outputs are on the same order of magnitude will not pay a lot of mass, even if they do increase the UTXO set. This excludes the ability of a storage attack, since such an attack requires breaking large UTXOs into small UTXOs. This is why using the arithmetic mean to credit and the harmonic mean to charge makes sense: The arithmetic mean is the same for any two sets of values of the same size and sum, it completely "forgets" how the values are distributed. On the other hand, the harmonic mean is extremely sensitive to how values are distributed and becomes very small if the smallest value is very small (by definition, the harmonic sum of $N>1$ values, one of which is $v$, is smaller than $vN$).

In [3], we formalize this intuitive interpretation by proving that this definition of `storage_mass` satisfies the "global quadraticity" property described above. A more (but not completely) formal version of our theorem is as follows: let $T$ be a set of transactions (say, transactions created by an attacker attempting a dust attack), say that there are $R$ UTXOs that are consumed by $T$ (that is, consumed by a transaction in $T$, but not created by any transaction in $T$), and $L$ UTXOs created by $T$ (that is, created by a transaction in $T$, but not consumed by a transaction in $T$). Finally, let `budget` be the sum of values of $I$, then
$$\sum_{\text{tx}\in T}\text{storage mass}\left(tx\right)\ge C\cdot\frac{\left(L-R\right)^{2}}{\text{budget}}\text{.}$$

Below we provide concrete illustrations of the consequences of this bound.

### Relaxed storage mass formula
Consider the following alternative definition:
$$\text{storage mass}^*(tx) = C\cdot\left(\frac{|O|}{H(O)} - \frac{|I|}{H(I)}\right)^+\text{.}$$

We call this the *relaxed formula*, as it treats inputs and outputs the same. With some work, one can see that the relaxed formula cannot substitute the standard formula. However, we show in [3] that the quadratic bound still holds when using `storage_mass*` for transactions satisfying $|O|\le|I|\le 2$ (or if $|O|=1$).

This relaxation might seem mild, but it actually has important practical implications. In particular, as we explain below, it allows a very simple algorithm for wallets to minimize storage mass.

**Remark**: Some might wonder whether we can apply the relaxation for *any* transaction with $|O|\le |I|$. The answer is that we believe it is, but are unable to prove it, and are convinced that proving it requires a completely different proof construction than the one we used. However, it does not make much of a difference, since a transaction with $|O|\le |I|$ can be broken into a small number of transactions with $|O| \le |I| \le 2$ with very small overhead.

# Security analysis and growth regulation
In this section, we consider the consequences of applying the storage mass policy, setting $C=10^{12}$. We consider an attacker seeking to increase the storage requirements by one gigabyte. For that, they would have to create $20$ million, or $2 \cdot 10^7$ new UTXO entries. We can now ask ourselves two questions: 1. How long would the attack last given a fixed budget? 2. How expensive the attack should be given it should last a fixed amount of time.

- **Fixed budget** Say the attacker has a budget of $20,000$ kas. That is, $2\cdot 10^4\cdot 10^8$ dworks. Plugging this into the bound, we get that $C\cdot growth^2/budget = (10^{12}\cdot 4 \cdot 10^{14})/(2 \cdot 10^4 \cdot 10^8) = 2 \cdot 10^{14}$. That is, the attack would cost $2 \cdot 10^{14}$ grams, which would take the full capacity of 400 million blocks. Hence, in 10BPS such an attack would require at least a year and a half, assuming the attacker uses 100% of the network throughput and the fees are negligible.

- **Fixed growth rate** Say the attacker wishes to increase the storage by a whole GB within a single day (again, assuming the attacker is given the entire network throughput for negligible fees). In 10BPS, the network has a throughput of a bit over $4\cdot 10^{11}$ grams per day. Substituting into the bound and rearranging we get $budget \ge C \cdot growth^2/\text{mass}$. Substituting $C = 10^{12}$, $growth = 2 \cdot 10^7$ and $\text{mass} = 4\cdot 10^{11}$ we get that the required budget is at least $10^{15}/4$ dworks, which is $2.5$ million kaspa.

Overall, the attack is either very slow or very expensive. How does this compare with the dust attack? That attack created 1185 UTXOs per block. In 10BPS, this means 50GB per day. Without the quadratic bound, the only limitation to such an attack is that each output must be at least $10,000$ dworks. In other words, assuming 10BPS, an attacker could increase the storage by 50GB in a single day for 100,000 kaspa. The computation above shows that, with a budget of 100,000 kaspa, it would take *750 years* to waste 50GB. Conversely, wasting 50GB in one day requires a budget of 125 *million* kaspa.

We can also use the bound to provide an absolute worst-case upper bound on the ***organic*** growth of the UTXO set. Assume for simplicity the total circulation is $20$ billion kaspa. In 10BPS, the daily mass capacity is about $4\cdot 10^{11}$. The quadratic bound implies that in $d$ days the storage can increase by at most $460\sqrt{d}$ gigabytes. This allows us to bound the storage increase as a function of the time that passed since the mass policy was implemented. During the first day, the storage can increase by at most one terabyte. During the first year: at most $10$ terabyte. During the first ten years: at most $25$ terabytes. Reaching $100$ terabyte will require almost 130 years, and $1$ peta will not happen before 13 thousand years. This is a *very* mild growth considering it bounds even the worst scenario possible: all Kaspa holders joining forces to increase the storage as much as possible, managing to apply the best strategy to do so.

# Quality of service

We discuss the implications of storage mass on common everyday transactions.

First consider a transaction $\text{tx}$ with a single $100$ kaspa input, and two near-equal outputs of values $\sim 50$ kaspa. We can compute that
$$\text{storage mass}(\text{tx}) \approx 2C\cdot \left(\frac{2}{50 \cdot 10^8} - \frac{1}{100 \cdot 10^8}\right) = 300$$

In contrast, one can check that $\text{compute mass}(\text{tx}) \ge 2000$, so we see that the total mass is not changed, despite the fact that $\text{tx}$ is a relatively low-value transaction with more outputs than inputs.

More generally, we see that the `storage_mass` becomes dominant only when relatively small inputs emerge (where the exact threshold is inversely proportional to the total budget of the transaction). Still, even in everyday use, we see that small outputs can emerge. Small UTXOs commonly appear as change or as micropayments, and however we set $C$, we should account for the possibility that a standard payment from a standard wallet could exceed this threshold, affecting the QoS of everyday users. In section [Wallet algorithm](#wallet-algorithm) we will show how to compose transactions for sending arbitrarily small values, and in section [Micropayments](#micropayments) we will discuss strategies for mitigating this cost altogether.


### Compounding transactions
A transaction $\text{tx}$ is *compounding* if $|O| \le |I|$ and all values in $O$ are equal, namely $=\text{budget}/|O|$ (most commonly, we have that $|O|=1$). Since all values in $O$ are the same we have that $H(O) = A(O) = \text{budget}/|O|$ and we get that
$$\text{storage mass}(\text{tx}) = C\cdot\left(\frac{|O|}{A(O)} - \frac{|I|}{A(I)}\right)^+ = C\cdot\left(\frac{|O|^2 - |I|^2}{\text{budget}}\right)^+ = 0\text{.}$$
Hence, compounding several outputs into an *equal or smaller* number of outputs *of equal value* will never incur storage mass. This is true *regardless of the magnitude* of the output values.

It is worth deliberating a bit on how fees affect this phenomenon. The presence of a fee modifies the mass equation as so:
$$\text{storage mass}\left(\text{tx}\right)=C\cdot\left(\frac{\left|O\right|^{2}}{\text{budget}-\text{fee}}-\frac{\left|I\right|^{2}}{\text{budget}}\right)$$
and after some rearrangement, one can see that the storage mass becomes positive if and only if
$$\left(1-\left(\frac{\left|O\right|}{\left|I\right|}\right)^{2}\right)<\frac{\text{fee}}{\text{budget}}$$

It is nice to notice that the condition above completely depends on the number of inputs and outputs and the fee-to-budget ratio, not the actual values or even $C$. However, in scenarios where the storage mass is positive, its actual value does depend on all of these quantities. The first example that jumps to the eye is the case $|I|=|O|$, where clearly *any* positive fee will beget positive storage mass. However, one can see numerically that unless the values are very small and/or the fee is very large, the storage mass will still be smaller than the computation mass.

In the case $|O| = 1$ the condition becomes
$$\left(1-\frac{1}{\left|I\right|^{2}}\right)<\frac{\text{fee}}{\text{budget}}$$
So even for $|I|=2$, the storage mass vanishes unless the fee is at least three-quarters of the entire value.

The overall takeaway of the analysis is that compounding is treated "nicely" by the storage mass function, even in the presence of fees. 

### Exchanges and pools
In the long term, an exchange can be considered a highly active wallet whose deposit and withdrawal volumes are roughly the same. That is, the values of deposits are distributed roughly the same as those of withdrawals. By matching inputs with outputs of the same magnitude, exchanges can keep the storage mass low even in a future where typical transaction have low (say, sub-kaspa) values.

Similarly, pools can remove most storage mass by setting the cashout limit to be larger than a typical coinbase output.

### Micropayments
A comprehensive solution must also consider scenarios where everyday users wish to spend very small values (much smaller than $0.1$ kaspa). We discuss how this can be achieved, assuming all wallets hold at least 1 kaspa (i.e. $10^8$ dworks).

Consider an eccentric millionaire with a penchant for ice cream. Every morning, our hero grabs a wallet containing a single UTXO of at least $1000$ kaspa, and goes to the stand to buy a vanilla cone for a comfortable price of $0.1$ kaspa. How should the millionaire pay for the tasty treat? 

Consider a transaction $\text{tx}$ with a single input of value $V$ and two outputs of values $v$ and $V-v$ where $v\ll V$, then it follows that
$$\text{storage mass}\left(\text{tx}\right)\approx C\cdot\left(\frac{1}{V-v}+\frac{1}{v}-\frac{1}{V}\right)\approx\frac{C}{v}\text{.}$$
In particular, in order to pay the vendor $0.1$ kaspa, and the change back to himself, the millionaire would have to create a transaction whose mass is around $100,000$ grams. This is one-fifth of the capacity of a block. That is, the fees for this transaction would be more than 100 times more expensive than "standard" transactions (whose storage mass is lower than their computation mass)! Paying as much for a single transaction might be excusable as a one-time last resort, but users would not (and should not) agree to pay it daily, not even a millionaire.

In an account-based model, this problem does not exist: the millionaire could simply pay *some* of the money in his wallet to the merchant's wallet. The crux is that this behavior can be emulated in the UTXO model by creating a mutually signed transaction. The vendor and the millionaire create a transaction together containing with two $1000$ kaspa inputs, one spent by the vendor and the other spent by the millionaire, and two outputs, one paying $1000.1$ kaspa to the vendor and the other paying $999.9$ kaspa to the millionaire. This facilitates the payment while removing the storage mass costs. Note that the UTXO available to the merchant need not exactly match in value to the millionaire's wallet. Even using a UTXO of $10$ kaspa, the outputs are $999.9$ and $10.1$. Since the smallest output is much larger, the mass would turn out much smaller. One can compute that for these exact numbers the mass turns out to be just below a kilogram, which is inconsequential, given that the computation mass is typically twice as much (recall that the total mass is the maximum of the two, not the sum). It is a reasonably good approximation to say that the storage mass becomes larger than the computation mass if the vendor's UTXO is at least two hundred times smaller (or larger) than the millionaire's.

The downside to this solution is that the merchant must constantly have a hot wallet available and cooperate with the customer to create a mutually signed transaction. In a future KIP, we will specify *auto-compunding* UTXOs that allow anyone to add to their balance without requiring the owner's signature. Auto-compounding UTXOs will allow the millionaire to purchase ice cream as described above, using her wallet alone.

### Wallet algorithm
In this section we provide an optimal algorithm for making a payment of a given value using a given set of UTXOs. The algorithm is "optimal" in the sense that it minimizes the number of transactions required to make this payment and the overall mass they consume. In scenarios where the storage mass is lower than the computation mass, the current algorithm should be used instead.

We assume that there is a maximal storage mass $M$ a single transaction could have. The value $M$ can be either the common mempool bound (currently set to 100,000 grams), or user specified. We also assume there is a conversion ratio $r$ of mass to fee. The value of $r$ is usually obtained by the wallet software by monitoring the network usage. Hence a transaction of mass $m$ will pay a fee of $rm$. We let $F=rM$ denote the fee of a maximally heavy transaction.

Assume the user desires to make a payment with value $P$ and that we already collected sufficient inputs $I$ such that $\sum_{v \in I}v \ge P + F$. Furthermore, assume that $P \gg F$ (otherwise it might not be worth making this payment altogether). The algorithm assumes that a transaction composed of $I$ and $O = \lbrace P, \sum_{v \in I}v - P - F\rbrace$ has $\text{compute mass} \le M$ but $\text{storage mass} > M$. Otherwise, if compute mass is too large, it should be solved using the current methods of compounding by building a transaction tree. Recall that compounding transactions will never have storage mass, so there's never a need to solve both objectives in parallel. Eventually we arrive at a root transaction where compute mass is low enough, at which point storage mass can be dealt with if needed. 

**Single step optimization** We begin by solving a single step of the process. The question we ask is “Given a mass bound $M$ and a set of inputs $I$, what is the minimum payment value possible without surpassing $M$?”. Or in more mathematical terms “What is the maximum asymmetry we can create between the 2 outputs given the constrains?”. Denote $N$ to be the negative component of the storage mass as described by the relaxed formula. Note that $I$ and the number of outputs (which is known to be 2 in this case), are sufficient for calculating this part. Let $T=\sum_{v \in I}v - F$ denote the total outputs value. We need to solve the following equation: $M = C/T\alpha + C/T(1-\alpha) - N$, where $\alpha \in (0, 1)$. Reorganizing terms we get $(M+N)T/C = 1/\alpha + 1/(1-\alpha)$. Let $D = (M+N)T/C$. Reorganizing terms further we arrive at the quadratic equation $D\alpha^2 - D\alpha + 1 = 0$ which solutions for are $\alpha = (D \pm \sqrt{D^2 - 4D})/2D$. Note that from symmetry of $\alpha, (1-\alpha)$ both solutions of the equation essentially give the same answer.

**Iterative process** Using this single step optimization we now describe an iterative algorithm for composing the chain of transactions required for making a micropayment: 

```python
def BuildTxs(inputs, payment):
    txs = []
    while storage_mass(I=inputs, O=[payment, sum(inputs) - payment - F]) > M:
        T = sum(inputs) - F
        N = negative_mass(inputs, 2) 
        D = (M + N)T/C
        alpha = (D - sqrt(D^2 - 4D))/2D                         # single step optimization, taking the smaller solution 
        outputs = [ceiling(alpha * T), T - ceiling(alpha * T)]  # round up in order to not increase the mass above M
        txs.append((inputs, outputs))
        inputs = outputs
    txs.append((inputs, [payment, sum(inputs) - payment - F]))
    return txs
```

**Remarks**
- In all iterations of the while loop (except maybe for the first), $I=O=2$ 
- Without the relaxation which uses the harmonic mean over the inputs (in the 2:2 case), the loop would not converge. The arithmetic averaging done over the inputs would yield the same $N$ value over and over
- In all intermediate transactions built within the loop, the recipient for both outputs should be the change address of the *sender* 
- In the final transaction built, it is possible that the actual fee can be decreased below $F$ depending on the final mass

# Implementation details

### Implementing as a P2P rule
Note that the new mass formula can be implemented as a mempool policy rather than a consensus rule. This has the benefit of not requiring a fork to roll out, but the disadvantage of trusting the miners to follow this policy. Due to the urgency of the update, we suggest to initially roll it out as a a P2P rule, while also including it in the nearest upcoming hard-fork.

### Consensus (hard-fork)
Here we discuss an important subtlety in computing the mass, and how it could be solved by slightly changing the structure of a transaction.

**Background** The block validation in Kaspa is divided into three phases:
- Header validation: the block header has the required form, points to known blocks, satisfies the difficulty target, etc.
- Data validation: the block body has the required form, as do all transactions therein, and in particular it satisfies the mass limit
- Transaction validation: all transactions have valid signatures and spend existing UTXOs.
Perhaps surprisingly, a block only has to pass the first two phases to be considered valid. The reason for that is that the third part of validation does not rely on the block data alone, but requires us to compute the UTXO set from the point of view of that block. For efficiency reasons, we only want to compute the UTXO set for selected-chain blocks. This tension is resolved by having this "intermediate status" where a block could be perfectly valid, but disqualified from being in the selected-chain. In particular, future blocks pointing at this block will process any valid transactions that appear therein (otherwise they will also be disqualified from the selected-chain).

**The Problem** The values of UTXOs are not explicitly written inside the transaction but are rather recovered from the UTXO set. So in order to compute the storage mass, we must compute the UTXO state of the block. This contradicts our ambition to validate the block during the data validation phase while avoiding having to compute its UTXO state.

**Solution** We adapt an idea we already used for fixing an issue with the `sig_op_count` opcode (see [4]). We require transactions to have an explicit `mass` field. During the data validation phase, the client only verifies that the sum of masses does not exceed the block mass limit. Verifying that the stated mass values are consistent with the formula is only performed for chain blocks and deferred to the transaction validation phase. Transactions with false mass values are considered invalid, and blocks containing them are disqualified from the selected-chain. We suggest that wallets leave the mass field empty, and miners fill it for them, as miners will want to verify the mass of all transactions they include to avoid an invalid transaction that will disqualify their block.

**Remark** To make the stated mass binding, it is crucial that it is used when computing the transaction hash. Hence, this change requires a hard-fork. However, we suggest that the logic for computing a transaction *ID* should ignore this field to maintain backward compatibility. This way, any ecosystem software that uses a transaction ID would not have to adapt its lookup logic after the hard-fork is rolled.

*Important note:* Once the hard-fork is applied, all mining software must update their Grpc interface to a version that includes the new transaction field. Otherwise, they would create invalid blocks.

### Calculation accuracy
Like any consensus component, storage mass calculation must use only integers and cannot rely of floating-point arithmetics. Regarding the storage formula, this means that the constant $C$ must be computed within each fraction, otherwise the loss of precision will render the calculation useless.

### Current status
This proposal is already implemented by the Kaspa on Rust codebase ([PR](https://github.com/kaspanet/rusty-kaspa/pull/379)). Currently applied on the mempool level for all networks (mainnet, testnet 10, testnet 11). It is also implemented on the consensus level, but only activated for testnet 11 (TN11). The current implementation however is slightly different. Particularly, it sums the storage mass with the compute mass instead of using max, and it also does not implement the relaxed formula. The implementation should be updated shortly to reflect this final proposal. To avoid code clutter and multiple versions, we suggest hard-forking or restarting TN11 with the fixed rules. 

# References
[1] “A very dusty yom-kippur” https://medium.com/@shai.wyborski/a-very-dusty-yom-kippur-dust-attack-post-mortem-faa11804e37

[2] Kaspad go-lang dust patch: https://github.com/kaspanet/kaspad/pull/2244

[3] Unpublished paper on state growth regulation in permissionless systems, this reference will be updated once the paper is published online.

[4] Kaspa Security Patch and Hard Fork — September 2022: https://medium.com/@michaelsuttonil/kaspa-security-patch-and-hard-fork-september-2022-12da617b0094
